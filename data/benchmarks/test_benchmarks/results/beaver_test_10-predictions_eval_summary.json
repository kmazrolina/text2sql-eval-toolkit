{
  "llm_judge_config": {
    "model": {
      "id": "wxai:meta-llama/llama-3-3-70b-instruct",
      "decoding_method": "greedy",
      "max_new_tokens": 512
    },
    "prompt_template": "You are an expert SQL evaluator. Your task is to assess whether a predicted SQL query correctly answers a natural language question, given the database schema and the ground truth SQL and result.\n\nPlease consider the following:\n- The predicted SQL may differ from the ground truth but still be valid.\n- The result dataframes may differ due to ambiguity in the question or schema.\n- Use your judgment to determine if the predicted SQL is a reasonable interpretation.\n\nRespond with one of the following verdicts:\n- \"Yes\" if the predicted SQL is correct (score: 1)\n- \"Maybe\" if the predicted SQL is possibly correct (score: 0.5)\n- \"No\" if the predicted SQL is incorrect (score: 0)\n\nStart your response with the verdict (\"Yes\", \"Maybe\", \"No\") and then provide a detailed explanation for your decision.\n\n### Question:\n{question}\n\n### Original Prompt Used for SQL Generation:\n{generation_prompt}\n\n### Ground Truth SQL:\n```sql\n{ground_truth_sql}\n```\n\n### Ground Truth Result:\n{ground_truth_df}\n\n### Predicted SQL:\n```sql\n{predicted_sql}\n```\n\n### Predicted Result (if any):\n{predicted_df}\n\n### Verdict and Explanation:"
  },
  "wxai:meta-llama/llama-3-3-70b-instruct-greedy-zero-shot-chatapi": {
    "execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "non_empty_execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "subset_non_empty_execution_accuracy": {
      "average": 0.1,
      "stddev": 0.31622776601683794
    },
    "logic_execution_accuracy": {
      "average": 0.1,
      "stddev": 0.31622776601683794
    },
    "bird_execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "is_sqlglot_parsable": {
      "average": 1.0,
      "stddev": 0.0
    },
    "is_sqlparse_parsable": {
      "average": 1.0,
      "stddev": 0.0
    },
    "sqlglot_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sqlglot_optimized_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sqlparse_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sql_exact_match": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sql_syntactic_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "eval_error": {
      "average": 0.0,
      "stddev": 0.0
    },
    "df_error": {
      "average": 0.7,
      "stddev": 0.4830458915396479
    },
    "prompt_tokens": {
      "average": 48656.7,
      "stddev": 13969.088263018455
    },
    "completion_tokens": {
      "average": 236.4,
      "stddev": 258.85826237537793
    },
    "total_tokens": {
      "average": 48893.1,
      "stddev": 14117.881257862708
    },
    "inference_time_ms": {
      "average": 28217.683,
      "stddev": 13520.960261883152
    },
    "execution_time_ms": {
      "average": 303.9,
      "stddev": 66.93602243934129
    },
    "llm_score": {
      "average": 0.2,
      "stddev": 0.42163702135578396
    },
    "sum_total_tokens": 488931,
    "sum_prompt_tokens": 486567,
    "sum_completion_tokens": 2364,
    "sum_inference_time_ms": 282176.83,
    "sum_execution_time_ms": 3039.0,
    "num_records": 10,
    "num_predictions": 10,
    "num_evaluated": 10,
    "num_eval_errors": 0,
    "num_df_errors": 7,
    "num_inference_errors": 0,
    "num_correct_non_empty_execution_accuracy": 0,
    "num_correct_subset_non_empty_execution_accuracy": 1,
    "num_correct_llm": 2,
    "num_llm_judge_errors": 0
  },
  "wxai:ibm/granite-4-h-small-greedy-zero-shot-chatapi": {
    "execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "non_empty_execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "subset_non_empty_execution_accuracy": {
      "average": 0.1,
      "stddev": 0.31622776601683794
    },
    "logic_execution_accuracy": {
      "average": 0.1,
      "stddev": 0.31622776601683794
    },
    "bird_execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "is_sqlglot_parsable": {
      "average": 0.8,
      "stddev": 0.4216370213557839
    },
    "is_sqlparse_parsable": {
      "average": 0.9,
      "stddev": 0.31622776601683794
    },
    "sqlglot_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sqlglot_optimized_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sqlparse_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sql_exact_match": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sql_syntactic_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "eval_error": {
      "average": 0.0,
      "stddev": 0.0
    },
    "df_error": {
      "average": 0.8,
      "stddev": 0.42163702135578396
    },
    "prompt_tokens": {
      "average": 42086.0,
      "stddev": 13410.844807261188
    },
    "completion_tokens": {
      "average": 285.1,
      "stddev": 240.86499215212748
    },
    "total_tokens": {
      "average": 42371.1,
      "stddev": 13512.758101142786
    },
    "inference_time_ms": {
      "average": 15198.257000000001,
      "stddev": 9285.572098447647
    },
    "execution_time_ms": {
      "average": 274.019,
      "stddev": 408.30466866054803
    },
    "llm_score": {
      "average": 0.1,
      "stddev": 0.3333333333333333
    },
    "sum_total_tokens": 423711,
    "sum_prompt_tokens": 420860,
    "sum_completion_tokens": 2851,
    "sum_inference_time_ms": 151982.57,
    "sum_execution_time_ms": 2740.19,
    "num_records": 10,
    "num_predictions": 9,
    "num_evaluated": 10,
    "num_eval_errors": 0,
    "num_df_errors": 8,
    "num_inference_errors": 1,
    "num_correct_non_empty_execution_accuracy": 0,
    "num_correct_subset_non_empty_execution_accuracy": 1,
    "num_correct_llm": 1,
    "num_llm_judge_errors": 0
  },
  "wxai:meta-llama/llama-4-maverick-17b-128e-instruct-fp8-greedy-zero-shot-chatapi": {
    "execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "non_empty_execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "subset_non_empty_execution_accuracy": {
      "average": 0.1,
      "stddev": 0.31622776601683794
    },
    "logic_execution_accuracy": {
      "average": 0.1,
      "stddev": 0.31622776601683794
    },
    "bird_execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "is_sqlglot_parsable": {
      "average": 1.0,
      "stddev": 0.0
    },
    "is_sqlparse_parsable": {
      "average": 1.0,
      "stddev": 0.0
    },
    "sqlglot_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sqlglot_optimized_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sqlparse_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sql_exact_match": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sql_syntactic_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "eval_error": {
      "average": 0.0,
      "stddev": 0.0
    },
    "df_error": {
      "average": 0.7,
      "stddev": 0.4830458915396479
    },
    "prompt_tokens": {
      "average": 48484.9,
      "stddev": 13930.092441665034
    },
    "completion_tokens": {
      "average": 343.0,
      "stddev": 269.1567077621015
    },
    "total_tokens": {
      "average": 48827.9,
      "stddev": 14028.319313683542
    },
    "inference_time_ms": {
      "average": 6961.101000000001,
      "stddev": 3361.2384635212125
    },
    "execution_time_ms": {
      "average": 418.78099999999995,
      "stddev": 629.4020430006034
    },
    "llm_score": {
      "average": 0.2,
      "stddev": 0.42163702135578396
    },
    "sum_total_tokens": 488279,
    "sum_prompt_tokens": 484849,
    "sum_completion_tokens": 3430,
    "sum_inference_time_ms": 69611.01,
    "sum_execution_time_ms": 4187.81,
    "num_records": 10,
    "num_predictions": 10,
    "num_evaluated": 10,
    "num_eval_errors": 0,
    "num_df_errors": 7,
    "num_inference_errors": 0,
    "num_correct_non_empty_execution_accuracy": 0,
    "num_correct_subset_non_empty_execution_accuracy": 1,
    "num_correct_llm": 2,
    "num_llm_judge_errors": 0
  },
  "wxai:openai/gpt-oss-120b-greedy-zero-shot-chatapi": {
    "execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "non_empty_execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "subset_non_empty_execution_accuracy": {
      "average": 0.1,
      "stddev": 0.31622776601683794
    },
    "logic_execution_accuracy": {
      "average": 0.1,
      "stddev": 0.31622776601683794
    },
    "bird_execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "is_sqlglot_parsable": {
      "average": 0.8,
      "stddev": 0.4216370213557839
    },
    "is_sqlparse_parsable": {
      "average": 1.0,
      "stddev": 0.0
    },
    "sqlglot_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sqlglot_optimized_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sqlparse_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sql_exact_match": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sql_syntactic_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "eval_error": {
      "average": 0.0,
      "stddev": 0.0
    },
    "df_error": {
      "average": 0.4,
      "stddev": 0.5163977794943222
    },
    "prompt_tokens": {
      "average": 48605.6,
      "stddev": 13937.746033304269
    },
    "completion_tokens": {
      "average": 827.9,
      "stddev": 239.3084713177627
    },
    "total_tokens": {
      "average": 49433.5,
      "stddev": 14129.053206071523
    },
    "inference_time_ms": {
      "average": 11729.675000000001,
      "stddev": 2325.2453785508796
    },
    "execution_time_ms": {
      "average": 1107.508,
      "stddev": 905.2972855513634
    },
    "llm_score": {
      "average": 0.5,
      "stddev": 0.5270462766947299
    },
    "sum_total_tokens": 494335,
    "sum_prompt_tokens": 486056,
    "sum_completion_tokens": 8279,
    "sum_inference_time_ms": 117296.75,
    "sum_execution_time_ms": 11075.08,
    "num_records": 10,
    "num_predictions": 10,
    "num_evaluated": 10,
    "num_eval_errors": 0,
    "num_df_errors": 4,
    "num_inference_errors": 0,
    "num_correct_non_empty_execution_accuracy": 0,
    "num_correct_subset_non_empty_execution_accuracy": 1,
    "num_correct_llm": 5,
    "num_llm_judge_errors": 0
  },
  "wxai:openai/gpt-oss-120b-agentic-baseline0-3attempts": {
    "execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "non_empty_execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "subset_non_empty_execution_accuracy": {
      "average": 0.1,
      "stddev": 0.31622776601683794
    },
    "logic_execution_accuracy": {
      "average": 0.1,
      "stddev": 0.31622776601683794
    },
    "bird_execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "is_sqlglot_parsable": {
      "average": 0.8,
      "stddev": 0.4216370213557839
    },
    "is_sqlparse_parsable": {
      "average": 0.8,
      "stddev": 0.4216370213557839
    },
    "sqlglot_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sqlglot_optimized_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sqlparse_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sql_exact_match": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sql_syntactic_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "eval_error": {
      "average": 0.0,
      "stddev": 0.0
    },
    "df_error": {
      "average": 0.3,
      "stddev": 0.48304589153964794
    },
    "prompt_tokens": {
      "average": 99914.7,
      "stddev": 77693.36755912809
    },
    "completion_tokens": {
      "average": 1427.3,
      "stddev": 1038.1838897270989
    },
    "total_tokens": {
      "average": 101342.0,
      "stddev": 78666.3367055227
    },
    "inference_time_ms": {
      "average": 64047.335,
      "stddev": 67757.8173672905
    },
    "llm_score": {
      "average": 0.5,
      "stddev": 0.5270462766947299
    },
    "sum_total_tokens": 1013420,
    "sum_prompt_tokens": 999147,
    "sum_completion_tokens": 14273,
    "sum_inference_time_ms": 640473.35,
    "num_records": 10,
    "num_predictions": 10,
    "num_evaluated": 10,
    "num_eval_errors": 0,
    "num_df_errors": 3,
    "num_inference_errors": 0,
    "num_correct_non_empty_execution_accuracy": 0,
    "num_correct_subset_non_empty_execution_accuracy": 1,
    "num_correct_llm": 5,
    "num_llm_judge_errors": 0
  },
  "wxai:openai/gpt-oss-120b-agentic-baseline1-3attempts": {
    "execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "non_empty_execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "subset_non_empty_execution_accuracy": {
      "average": 0.1,
      "stddev": 0.31622776601683794
    },
    "logic_execution_accuracy": {
      "average": 0.1,
      "stddev": 0.31622776601683794
    },
    "bird_execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "is_sqlglot_parsable": {
      "average": 0.9,
      "stddev": 0.31622776601683794
    },
    "is_sqlparse_parsable": {
      "average": 1.0,
      "stddev": 0.0
    },
    "sqlglot_equivalence": {
      "average": 0.1,
      "stddev": 0.31622776601683794
    },
    "sqlglot_optimized_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sqlparse_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sql_exact_match": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sql_syntactic_equivalence": {
      "average": 0.1,
      "stddev": 0.31622776601683794
    },
    "eval_error": {
      "average": 0.0,
      "stddev": 0.0
    },
    "df_error": {
      "average": 0.1,
      "stddev": 0.31622776601683794
    },
    "prompt_tokens": {
      "average": 70079.5,
      "stddev": 42857.0834758451
    },
    "completion_tokens": {
      "average": 1133.5,
      "stddev": 896.3951571588157
    },
    "total_tokens": {
      "average": 71213.0,
      "stddev": 43737.07544965585
    },
    "inference_time_ms": {
      "average": 34144.945,
      "stddev": 46301.334059948174
    },
    "llm_score": {
      "average": 0.8,
      "stddev": 0.42163702135578396
    },
    "sum_total_tokens": 712130,
    "sum_prompt_tokens": 700795,
    "sum_completion_tokens": 11335,
    "sum_inference_time_ms": 341449.45,
    "num_records": 10,
    "num_predictions": 10,
    "num_evaluated": 10,
    "num_eval_errors": 0,
    "num_df_errors": 1,
    "num_inference_errors": 0,
    "num_correct_non_empty_execution_accuracy": 0,
    "num_correct_subset_non_empty_execution_accuracy": 1,
    "num_correct_llm": 8,
    "num_llm_judge_errors": 0
  },
  "wxai:openai/gpt-oss-120b-agentic-baseline2-3attempts": {
    "execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "non_empty_execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "subset_non_empty_execution_accuracy": {
      "average": 0.1,
      "stddev": 0.31622776601683794
    },
    "logic_execution_accuracy": {
      "average": 0.1,
      "stddev": 0.31622776601683794
    },
    "bird_execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "is_sqlglot_parsable": {
      "average": 1.0,
      "stddev": 0.0
    },
    "is_sqlparse_parsable": {
      "average": 1.0,
      "stddev": 0.0
    },
    "sqlglot_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sqlglot_optimized_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sqlparse_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sql_exact_match": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sql_syntactic_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "eval_error": {
      "average": 0.0,
      "stddev": 0.0
    },
    "df_error": {
      "average": 0.1,
      "stddev": 0.31622776601683794
    },
    "prompt_tokens": {
      "average": 72624.3,
      "stddev": 52643.280919896235
    },
    "completion_tokens": {
      "average": 1143.1,
      "stddev": 822.7085956359842
    },
    "total_tokens": {
      "average": 73767.4,
      "stddev": 53451.80253940096
    },
    "inference_time_ms": {
      "average": 25216.785,
      "stddev": 39192.296137060526
    },
    "llm_score": {
      "average": 0.7,
      "stddev": 0.4830458915396479
    },
    "sum_total_tokens": 737674,
    "sum_prompt_tokens": 726243,
    "sum_completion_tokens": 11431,
    "sum_inference_time_ms": 252167.85,
    "num_records": 10,
    "num_predictions": 10,
    "num_evaluated": 10,
    "num_eval_errors": 0,
    "num_df_errors": 1,
    "num_inference_errors": 0,
    "num_correct_non_empty_execution_accuracy": 0,
    "num_correct_subset_non_empty_execution_accuracy": 1,
    "num_correct_llm": 7,
    "num_llm_judge_errors": 0
  },
  "wxai:openai/gpt-oss-120b-agentic-baseline3-3attempts": {
    "execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "non_empty_execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "subset_non_empty_execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "logic_execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "bird_execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "is_sqlglot_parsable": {
      "average": 0.9,
      "stddev": 0.31622776601683794
    },
    "is_sqlparse_parsable": {
      "average": 1.0,
      "stddev": 0.0
    },
    "sqlglot_equivalence": {
      "average": 0.1,
      "stddev": 0.31622776601683794
    },
    "sqlglot_optimized_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sqlparse_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sql_exact_match": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sql_syntactic_equivalence": {
      "average": 0.1,
      "stddev": 0.31622776601683794
    },
    "eval_error": {
      "average": 0.0,
      "stddev": 0.0
    },
    "df_error": {
      "average": 0.4,
      "stddev": 0.5163977794943223
    },
    "prompt_tokens": {
      "average": 107714.0,
      "stddev": 44547.746901498846
    },
    "completion_tokens": {
      "average": 1726.8,
      "stddev": 772.175109673965
    },
    "total_tokens": {
      "average": 109440.8,
      "stddev": 45233.07956892709
    },
    "inference_time_ms": {
      "average": 92155.06499999999,
      "stddev": 58696.28246928387
    },
    "llm_score": {
      "average": 0.4,
      "stddev": 0.5163977794943222
    },
    "sum_total_tokens": 1094408,
    "sum_prompt_tokens": 1077140,
    "sum_completion_tokens": 17268,
    "sum_inference_time_ms": 921550.65,
    "num_records": 10,
    "num_predictions": 10,
    "num_evaluated": 10,
    "num_eval_errors": 0,
    "num_df_errors": 4,
    "num_inference_errors": 0,
    "num_correct_non_empty_execution_accuracy": 0,
    "num_correct_subset_non_empty_execution_accuracy": 0,
    "num_correct_llm": 4,
    "num_llm_judge_errors": 0
  },
  "wxai:openai/gpt-oss-120b-agentic-baseline4-3attempts": {
    "execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "non_empty_execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "subset_non_empty_execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "logic_execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "bird_execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "is_sqlglot_parsable": {
      "average": 0.4,
      "stddev": 0.0
    },
    "is_sqlparse_parsable": {
      "average": 0.4,
      "stddev": 0.0
    },
    "sqlglot_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sqlglot_optimized_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sqlparse_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sql_exact_match": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sql_syntactic_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "eval_error": {
      "average": 0.0,
      "stddev": 0.0
    },
    "df_error": {
      "average": 0.2,
      "stddev": 0.5773502691896257
    },
    "prompt_tokens": {
      "average": 86940.0,
      "stddev": 134562.89296335253
    },
    "completion_tokens": {
      "average": 819.3,
      "stddev": 896.5137570983876
    },
    "total_tokens": {
      "average": 87759.3,
      "stddev": 135281.73855421136
    },
    "inference_time_ms": {
      "average": 142662.667,
      "stddev": 224094.56644433786
    },
    "llm_score": {
      "average": 0.2,
      "stddev": 0.5773502691896257
    },
    "sum_total_tokens": 877593,
    "sum_prompt_tokens": 869400,
    "sum_completion_tokens": 8193,
    "sum_inference_time_ms": 1426626.67,
    "num_records": 10,
    "num_predictions": 10,
    "num_evaluated": 4,
    "num_eval_errors": 6,
    "num_df_errors": 8,
    "num_inference_errors": 0,
    "num_correct_non_empty_execution_accuracy": 0,
    "num_correct_subset_non_empty_execution_accuracy": 0,
    "num_correct_llm": 2,
    "num_llm_judge_errors": 0
  },
  "wxai:openai/gpt-oss-120b-agentic-baseline5-3attempts": {
    "execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "non_empty_execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "subset_non_empty_execution_accuracy": {
      "average": 0.1,
      "stddev": 0.7071067811865476
    },
    "logic_execution_accuracy": {
      "average": 0.1,
      "stddev": 0.7071067811865476
    },
    "bird_execution_accuracy": {
      "average": 0.0,
      "stddev": 0.0
    },
    "is_sqlglot_parsable": {
      "average": 0.2,
      "stddev": 0.0
    },
    "is_sqlparse_parsable": {
      "average": 0.2,
      "stddev": 0.0
    },
    "sqlglot_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sqlglot_optimized_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sqlparse_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sql_exact_match": {
      "average": 0.0,
      "stddev": 0.0
    },
    "sql_syntactic_equivalence": {
      "average": 0.0,
      "stddev": 0.0
    },
    "eval_error": {
      "average": 0.0,
      "stddev": 0.0
    },
    "df_error": {
      "average": 0.0,
      "stddev": 0.0
    },
    "prompt_tokens": {
      "average": 35289.4,
      "stddev": 36927.94454068626
    },
    "completion_tokens": {
      "average": 646.6,
      "stddev": 2064.7518010647186
    },
    "total_tokens": {
      "average": 35936.0,
      "stddev": 38992.696341750976
    },
    "inference_time_ms": {
      "average": 178808.315,
      "stddev": 24875.62765341314
    },
    "llm_score": {
      "average": 0.2,
      "stddev": 0.0
    },
    "sum_total_tokens": 359360,
    "sum_prompt_tokens": 352894,
    "sum_completion_tokens": 6466,
    "sum_inference_time_ms": 1788083.15,
    "num_records": 10,
    "num_predictions": 10,
    "num_evaluated": 2,
    "num_eval_errors": 8,
    "num_df_errors": 8,
    "num_inference_errors": 0,
    "num_correct_non_empty_execution_accuracy": 0,
    "num_correct_subset_non_empty_execution_accuracy": 1,
    "num_correct_llm": 2,
    "num_llm_judge_errors": 0
  }
}