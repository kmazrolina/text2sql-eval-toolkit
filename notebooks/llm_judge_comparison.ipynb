{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "325433af",
   "metadata": {},
   "source": [
    "# ðŸ§  Comparing LLM-as-judge Functions\n",
    "\n",
    "Our goal is to compare different prompts and settings for LLM-as-judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add6e75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Auto-reload for dev\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from text2sql_eval_toolkit.utils import (\n",
    "    get_benchmark_info,\n",
    "    get_default_eval_filename,\n",
    "    get_question,\n",
    ")\n",
    "from text2sql_eval_toolkit.analysis.report_tools import collect_results\n",
    "from text2sql_eval_toolkit.evaluation.llm_as_judge import (\n",
    "    load_llm_judge_config,\n",
    "    evaluate_sql_prediction_with_llm,\n",
    ")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables for LLM API credentials from project root\n",
    "# The .env file should be in the project root directory (works when cwd is project root or notebooks/)\n",
    "project_root = Path.cwd() if (Path.cwd() / \".env\").exists() else Path.cwd().parent\n",
    "env_path = project_root / '.env'\n",
    "load_dotenv(env_path, override=True)\n",
    "\n",
    "print(f\"File exists: {env_path.exists()}\")\n",
    "print(f\"Loading .env from: {env_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fafce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "# For Jupyter notebook display\n",
    "try:\n",
    "    from IPython.display import Markdown, display\n",
    "\n",
    "    JUPYTER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    JUPYTER_AVAILABLE = False\n",
    "\n",
    "\n",
    "def create_confusion_matrix(pred_eval_data, key1, key2, pipeline_id=None):\n",
    "    \"\"\"\n",
    "    Create a confusion matrix between two evaluation metrics.\n",
    "\n",
    "    Args:\n",
    "        pred_eval_data: Array of objects with predictions\n",
    "        key1: First evaluation metric key (e.g., \"llm_score\")\n",
    "        key2: Second evaluation metric key (e.g., \"llm_judge_no_gt_v1_score\")\n",
    "        pipeline_id: Specific pipeline ID to analyze (if None, uses ALL pipeline IDs)\n",
    "\n",
    "    Returns:\n",
    "        confusion_matrix: 2x2 numpy array\n",
    "        agreement_percentage: Percentage of agreement between the two metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # Collect pairs of scores\n",
    "    score_pairs = []\n",
    "\n",
    "    for item in pred_eval_data:\n",
    "        if \"predictions\" in item:\n",
    "            predictions = item[\"predictions\"]\n",
    "\n",
    "            # If no specific pipeline_id provided, use ALL available pipeline IDs\n",
    "            if pipeline_id is None:\n",
    "                pipeline_ids = list(predictions.keys())\n",
    "            else:\n",
    "                pipeline_ids = [pipeline_id] if pipeline_id in predictions else []\n",
    "\n",
    "            for pid in pipeline_ids:\n",
    "                if pid in predictions:\n",
    "                    evaluation = predictions[pid].get(\"evaluation\", {})\n",
    "\n",
    "                    if key1 in evaluation and key2 in evaluation:\n",
    "                        score1 = evaluation[key1]\n",
    "                        score2 = evaluation[key2]\n",
    "\n",
    "                        # Convert to binary (assuming 0.0 and 1.0 are the only values)\n",
    "                        binary1 = int(score1 > 0.5)  # Convert to 0 or 1\n",
    "                        binary2 = int(score2 > 0.5)  # Convert to 0 or 1\n",
    "\n",
    "                        score_pairs.append((binary1, binary2))\n",
    "\n",
    "    if not score_pairs:\n",
    "        print(f\"No data found for keys '{key1}' and '{key2}'\")\n",
    "        return None, None\n",
    "\n",
    "    # Create confusion matrix\n",
    "    # Format: [[TN, FP], [FN, TP]]\n",
    "    # Where True/False refers to key1, Positive/Negative refers to key2\n",
    "    confusion_matrix = np.zeros((2, 2), dtype=int)\n",
    "\n",
    "    for score1, score2 in score_pairs:\n",
    "        confusion_matrix[score1][score2] += 1\n",
    "\n",
    "    # Calculate agreement percentage\n",
    "    total_samples = len(score_pairs)\n",
    "    agreements = sum(1 for s1, s2 in score_pairs if s1 == s2)\n",
    "    agreement_percentage = (\n",
    "        (agreements / total_samples) * 100 if total_samples > 0 else 0\n",
    "    )\n",
    "\n",
    "    return confusion_matrix, agreement_percentage\n",
    "\n",
    "\n",
    "def print_confusion_matrix_analysis(pred_eval_data, key1, key2, pipeline_id=None):\n",
    "    \"\"\"\n",
    "    Print a detailed analysis of the confusion matrix between two evaluation metrics.\n",
    "    By default, analyzes ALL pipeline IDs combined.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"CONFUSION MATRIX ANALYSIS\")\n",
    "    print(f\"Key 1 (rows): {key1}\")\n",
    "    print(f\"Key 2 (cols): {key2}\")\n",
    "\n",
    "    if pipeline_id:\n",
    "        print(f\"Pipeline ID: {pipeline_id}\")\n",
    "    else:\n",
    "        # Show which pipeline IDs are being analyzed\n",
    "        pipeline_ids = set()\n",
    "        for item in pred_eval_data:\n",
    "            if \"predictions\" in item:\n",
    "                pipeline_ids.update(item[\"predictions\"].keys())\n",
    "        pipeline_ids = sorted(list(pipeline_ids))\n",
    "        print(f\"Pipeline IDs: ALL ({len(pipeline_ids)} total: {pipeline_ids})\")\n",
    "\n",
    "        # Show per-pipeline agreement summary\n",
    "        if len(pipeline_ids) > 1:\n",
    "            pipeline_stats = get_pipeline_agreement_summary(pred_eval_data, key1, key2)\n",
    "            if pipeline_stats:\n",
    "                print(f\"\\nPer-Pipeline Agreement Summary:\")\n",
    "                print(\n",
    "                    f\"{'Pipeline ID':<20} {'Samples':<8} {'Agreements':<12} {'Agreement %':<12}\"\n",
    "                )\n",
    "                print(f\"{'-' * 20} {'-' * 8} {'-' * 12} {'-' * 12}\")\n",
    "                for stats in pipeline_stats:\n",
    "                    print(\n",
    "                        f\"{stats['pipeline_id']:<20} {stats['total_samples']:<8} {stats['agreements']:<12} {stats['agreement_percentage']:<12.1f}%\"\n",
    "                    )\n",
    "\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    confusion_matrix, agreement_percentage = create_confusion_matrix(\n",
    "        pred_eval_data, key1, key2, pipeline_id\n",
    "    )\n",
    "\n",
    "    if confusion_matrix is None:\n",
    "        return\n",
    "\n",
    "    # Print confusion matrix\n",
    "    matrix_title = (\n",
    "        \"Combined Confusion Matrix:\" if pipeline_id is None else \"Confusion Matrix:\"\n",
    "    )\n",
    "    print(f\"\\n{matrix_title}\")\n",
    "    print(f\"                 {key2}\")\n",
    "    print(f\"                 0.0    1.0\")\n",
    "    print(f\"{key1:12} 0.0  {confusion_matrix[0][0]:4d}   {confusion_matrix[0][1]:4d}\")\n",
    "    print(f\"            1.0  {confusion_matrix[1][0]:4d}   {confusion_matrix[1][1]:4d}\")\n",
    "\n",
    "    # Calculate and print detailed statistics\n",
    "    total = np.sum(confusion_matrix)\n",
    "    both_zero = confusion_matrix[0][0]  # Both scores are 0.0\n",
    "    both_one = confusion_matrix[1][1]  # Both scores are 1.0\n",
    "    key1_high_key2_low = confusion_matrix[1][0]  # Key1=1.0, Key2=0.0\n",
    "    key1_low_key2_high = confusion_matrix[0][1]  # Key1=0.0, Key2=1.0\n",
    "\n",
    "    breakdown_title = (\n",
    "        \"Combined Detailed Breakdown:\" if pipeline_id is None else \"Detailed Breakdown:\"\n",
    "    )\n",
    "    print(f\"\\n{breakdown_title}\")\n",
    "    print(f\"Total samples: {total}\")\n",
    "    print(\n",
    "        f\"Both {key1}=0.0 and {key2}=0.0: {both_zero:4d} ({both_zero / total * 100:.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Both {key1}=1.0 and {key2}=1.0: {both_one:4d} ({both_one / total * 100:.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"{key1}=1.0, {key2}=0.0: {key1_high_key2_low:4d} ({key1_high_key2_low / total * 100:.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"{key1}=0.0, {key2}=1.0: {key1_low_key2_high:4d} ({key1_low_key2_high / total * 100:.1f}%)\"\n",
    "    )\n",
    "\n",
    "    agreement_title = (\n",
    "        \"Combined Agreement Analysis:\" if pipeline_id is None else \"Agreement Analysis:\"\n",
    "    )\n",
    "    print(f\"\\n{agreement_title}\")\n",
    "    print(f\"Agreement percentage: {agreement_percentage:.2f}%\")\n",
    "    print(f\"Disagreement percentage: {100 - agreement_percentage:.2f}%\")\n",
    "\n",
    "    return confusion_matrix, agreement_percentage\n",
    "\n",
    "\n",
    "def get_pipeline_agreement_summary(pred_eval_data, key1, key2):\n",
    "    \"\"\"\n",
    "    Get agreement statistics for each individual pipeline.\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries with pipeline stats\n",
    "    \"\"\"\n",
    "    # Get all available pipeline IDs\n",
    "    pipeline_ids = set()\n",
    "    for item in pred_eval_data:\n",
    "        if \"predictions\" in item:\n",
    "            pipeline_ids.update(item[\"predictions\"].keys())\n",
    "\n",
    "    pipeline_ids = sorted(list(pipeline_ids))\n",
    "    pipeline_stats = []\n",
    "\n",
    "    for pipeline_id in pipeline_ids:\n",
    "        confusion_matrix, agreement_percentage = create_confusion_matrix(\n",
    "            pred_eval_data, key1, key2, pipeline_id\n",
    "        )\n",
    "\n",
    "        if confusion_matrix is not None:\n",
    "            total_samples = np.sum(confusion_matrix)\n",
    "            agreements = (\n",
    "                confusion_matrix[0][0] + confusion_matrix[1][1]\n",
    "            )  # Both 0.0 + Both 1.0\n",
    "\n",
    "            pipeline_stats.append(\n",
    "                {\n",
    "                    \"pipeline_id\": pipeline_id,\n",
    "                    \"total_samples\": total_samples,\n",
    "                    \"agreements\": agreements,\n",
    "                    \"agreement_percentage\": agreement_percentage,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pipeline_stats\n",
    "\n",
    "\n",
    "def print_confusion_matrix_markdown(\n",
    "    pred_eval_data, key1, key2, pipeline_id=None, display_in_notebook=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Print a confusion matrix analysis in Markdown format for Jupyter notebooks.\n",
    "    By default, analyzes ALL pipeline IDs combined.\n",
    "\n",
    "    Args:\n",
    "        display_in_notebook: If True and in Jupyter, renders the markdown nicely.\n",
    "                           If False, just prints the raw markdown text.\n",
    "    \"\"\"\n",
    "\n",
    "    confusion_matrix, agreement_percentage = create_confusion_matrix(\n",
    "        pred_eval_data, key1, key2, pipeline_id\n",
    "    )\n",
    "\n",
    "    if confusion_matrix is None:\n",
    "        message = \"No data found for the specified keys.\"\n",
    "        if display_in_notebook and JUPYTER_AVAILABLE:\n",
    "            display(Markdown(f\"**Error:** {message}\"))\n",
    "        else:\n",
    "            print(message)\n",
    "        return\n",
    "\n",
    "    # Get pipeline info and per-pipeline stats\n",
    "    if pipeline_id:\n",
    "        pipeline_info = f\"**Pipeline ID:** `{pipeline_id}`\"\n",
    "        pipeline_summary_table = \"\"\n",
    "    else:\n",
    "        pipeline_ids = set()\n",
    "        for item in pred_eval_data:\n",
    "            if \"predictions\" in item:\n",
    "                pipeline_ids.update(item[\"predictions\"].keys())\n",
    "        pipeline_ids = sorted(list(pipeline_ids))\n",
    "        pipeline_info = f\"**Pipeline IDs:** ALL ({len(pipeline_ids)} total: {', '.join([f'`{pid}`' for pid in pipeline_ids])})\"\n",
    "\n",
    "        # Get per-pipeline agreement stats\n",
    "        pipeline_stats = get_pipeline_agreement_summary(pred_eval_data, key1, key2)\n",
    "\n",
    "        if pipeline_stats:\n",
    "            pipeline_summary_table = f\"\"\"\n",
    "### ðŸ“‹ Per-Pipeline Agreement Summary\n",
    "\n",
    "| Pipeline ID | Total Samples | Agreements | Agreement % |\n",
    "|-------------|:-------------:|:----------:|:-----------:|\n",
    "\"\"\"\n",
    "            for stats in pipeline_stats:\n",
    "                pipeline_summary_table += f\"| `{stats['pipeline_id']}` | {stats['total_samples']:,} | {stats['agreements']:,} | {stats['agreement_percentage']:.1f}% |\\n\"\n",
    "\n",
    "            pipeline_summary_table += \"\\n\"\n",
    "        else:\n",
    "            pipeline_summary_table = \"\"\n",
    "\n",
    "    # Calculate statistics\n",
    "    total = np.sum(confusion_matrix)\n",
    "    both_zero = confusion_matrix[0][0]\n",
    "    both_one = confusion_matrix[1][1]\n",
    "    key1_high_key2_low = confusion_matrix[1][0]\n",
    "    key1_low_key2_high = confusion_matrix[0][1]\n",
    "\n",
    "    # Generate Markdown\n",
    "    markdown = f\"\"\"## ðŸ” Confusion Matrix Analysis\n",
    "\n",
    "**Metrics Compared:**\n",
    "- **Key 1 (rows):** `{key1}`\n",
    "- **Key 2 (cols):** `{key2}`\n",
    "\n",
    "{pipeline_info}\n",
    "{pipeline_summary_table}\n",
    "### Confusion Matrix (Combined)\n",
    "\n",
    "|                | **{key2}=0.0** | **{key2}=1.0** |\n",
    "|----------------|:--------------:|:--------------:|\n",
    "| **{key1}=0.0** |      {both_zero:,}       |      {key1_low_key2_high:,}       |\n",
    "| **{key1}=1.0** |   {key1_high_key2_low:,}        |      {both_one:,}       |\n",
    "\n",
    "### ðŸ“Š Detailed Breakdown (Combined)\n",
    "\n",
    "| Category | Count | Percentage |\n",
    "|----------|------:|-----------:|\n",
    "| **Total samples** | {total:,} | 100.0% |\n",
    "| Both metrics = 0.0 | {both_zero:,} | {both_zero / total * 100:.1f}% |\n",
    "| Both metrics = 1.0 | {both_one:,} | {both_one / total * 100:.1f}% |\n",
    "| `{key1}`=1.0, `{key2}`=0.0 | {key1_high_key2_low:,} | {key1_high_key2_low / total * 100:.1f}% |\n",
    "| `{key1}`=0.0, `{key2}`=1.0 | {key1_low_key2_high:,} | {key1_low_key2_high / total * 100:.1f}% |\n",
    "\n",
    "### ðŸŽ¯ Agreement Analysis (Combined)\n",
    "\n",
    "- **Agreement:** {agreement_percentage:.2f}% ({both_zero + both_one:,} out of {total:,} samples)\n",
    "- **Disagreement:** {100 - agreement_percentage:.2f}% ({key1_high_key2_low + key1_low_key2_high:,} out of {total:,} samples)\"\"\"\n",
    "\n",
    "    # Display in notebook or print as text\n",
    "    if display_in_notebook and JUPYTER_AVAILABLE:\n",
    "        display(Markdown(markdown))\n",
    "    else:\n",
    "        print(markdown)\n",
    "\n",
    "    return confusion_matrix, agreement_percentage\n",
    "\n",
    "\n",
    "def analyze_all_pipeline_combinations(pred_eval_data, key1, key2):\n",
    "    \"\"\"\n",
    "    Analyze confusion matrix for all available pipeline IDs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get all available pipeline IDs\n",
    "    pipeline_ids = set()\n",
    "    for item in pred_eval_data:\n",
    "        if \"predictions\" in item:\n",
    "            pipeline_ids.update(item[\"predictions\"].keys())\n",
    "\n",
    "    pipeline_ids = sorted(list(pipeline_ids))\n",
    "\n",
    "    if not pipeline_ids:\n",
    "        print(\"No pipeline IDs found in the data\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nFound {len(pipeline_ids)} pipeline ID(s): {pipeline_ids}\")\n",
    "\n",
    "    # Analyze each pipeline\n",
    "    for pipeline_id in pipeline_ids:\n",
    "        print_confusion_matrix_analysis(pred_eval_data, key1, key2, pipeline_id)\n",
    "\n",
    "\n",
    "def analyze_all_pipeline_combinations_markdown(\n",
    "    pred_eval_data, key1, key2, display_in_notebook=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyze confusion matrix for all available pipeline IDs in Markdown format.\n",
    "\n",
    "    Args:\n",
    "        display_in_notebook: If True and in Jupyter, renders the markdown nicely.\n",
    "                           If False, just prints the raw markdown text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get all available pipeline IDs\n",
    "    pipeline_ids = set()\n",
    "    for item in pred_eval_data:\n",
    "        if \"predictions\" in item:\n",
    "            pipeline_ids.update(item[\"predictions\"].keys())\n",
    "\n",
    "    pipeline_ids = sorted(list(pipeline_ids))\n",
    "\n",
    "    if not pipeline_ids:\n",
    "        message = \"No pipeline IDs found in the data\"\n",
    "        if display_in_notebook and JUPYTER_AVAILABLE:\n",
    "            display(Markdown(f\"**Error:** {message}\"))\n",
    "        else:\n",
    "            print(message)\n",
    "        return\n",
    "\n",
    "    # Header markdown\n",
    "    header_markdown = f\"\"\"# ðŸ“‹ Pipeline-by-Pipeline Analysis\n",
    "\n",
    "**Found {len(pipeline_ids)} pipeline ID(s):** {\", \".join([f\"`{pid}`\" for pid in pipeline_ids])}\n",
    "\n",
    "---\"\"\"\n",
    "\n",
    "    if display_in_notebook and JUPYTER_AVAILABLE:\n",
    "        display(Markdown(header_markdown))\n",
    "    else:\n",
    "        print(header_markdown)\n",
    "\n",
    "    # Analyze each pipeline\n",
    "    for i, pipeline_id in enumerate(pipeline_ids, 1):\n",
    "        pipeline_header = f\"## Pipeline {i}: `{pipeline_id}`\"\n",
    "\n",
    "        if display_in_notebook and JUPYTER_AVAILABLE:\n",
    "            display(Markdown(pipeline_header))\n",
    "        else:\n",
    "            print(f\"\\n{pipeline_header}\\n\")\n",
    "\n",
    "        # Call the individual analysis function\n",
    "        print_confusion_matrix_markdown(\n",
    "            pred_eval_data, key1, key2, pipeline_id, display_in_notebook\n",
    "        )\n",
    "\n",
    "        # Add separator between pipelines (except for the last one)\n",
    "        if i < len(pipeline_ids):\n",
    "            if display_in_notebook and JUPYTER_AVAILABLE:\n",
    "                display(Markdown(\"---\"))\n",
    "            else:\n",
    "                print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "09599b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation file path: /data/code/text2sql/text2sql-eval-toolkit/data/results/bird_mini_dev_sqlite-predictions_eval.json\n"
     ]
    }
   ],
   "source": [
    "benchmark_id = \"bird_mini_dev_sqlite\"\n",
    "benchmark_info = get_benchmark_info(benchmark_id)\n",
    "predictions_path = benchmark_info[\"predictions_path\"]\n",
    "eval_path = Path(get_default_eval_filename(predictions_path))\n",
    "print(f\"Evaluation file path: {eval_path}\")\n",
    "pred_eval_data = json.load(eval_path.open(\"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "401d2b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## ðŸ” Confusion Matrix Analysis\n",
       "\n",
       "**Metrics Compared:**\n",
       "- **Key 1 (rows):** `llm_score`\n",
       "- **Key 2 (cols):** `llm_judge_no_gt_v1_score`\n",
       "\n",
       "**Pipeline IDs:** ALL (7 total: `wxai:ibm/granite-3-3-8b-instruct-greedy-zero-shot`, `wxai:ibm/granite-3-3-8b-instruct-greedy-zero-shot-chatapi`, `wxai:meta-llama/llama-3-3-70b-instruct-greedy-zero-shot`, `wxai:meta-llama/llama-3-3-70b-instruct-greedy-zero-shot-chatapi`, `wxai:meta-llama/llama-4-maverick-17b-128e-instruct-fp8-greedy-zero-shot`, `wxai:meta-llama/llama-4-maverick-17b-128e-instruct-fp8-greedy-zero-shot-chatapi`, `wxai:openai/gpt-oss-120b-greedy-zero-shot-chatapi`)\n",
       "\n",
       "### ðŸ“‹ Per-Pipeline Agreement Summary\n",
       "\n",
       "| Pipeline ID | Total Samples | Agreements | Agreement % |\n",
       "|-------------|:-------------:|:----------:|:-----------:|\n",
       "| `wxai:ibm/granite-3-3-8b-instruct-greedy-zero-shot` | 139 | 106 | 76.3% |\n",
       "| `wxai:ibm/granite-3-3-8b-instruct-greedy-zero-shot-chatapi` | 134 | 105 | 78.4% |\n",
       "| `wxai:meta-llama/llama-3-3-70b-instruct-greedy-zero-shot` | 141 | 109 | 77.3% |\n",
       "| `wxai:meta-llama/llama-3-3-70b-instruct-greedy-zero-shot-chatapi` | 137 | 114 | 83.2% |\n",
       "| `wxai:meta-llama/llama-4-maverick-17b-128e-instruct-fp8-greedy-zero-shot` | 138 | 118 | 85.5% |\n",
       "| `wxai:meta-llama/llama-4-maverick-17b-128e-instruct-fp8-greedy-zero-shot-chatapi` | 132 | 111 | 84.1% |\n",
       "| `wxai:openai/gpt-oss-120b-greedy-zero-shot-chatapi` | 128 | 108 | 84.4% |\n",
       "\n",
       "\n",
       "### Confusion Matrix (Combined)\n",
       "\n",
       "|                | **llm_judge_no_gt_v1_score=0.0** | **llm_judge_no_gt_v1_score=1.0** |\n",
       "|----------------|:--------------:|:--------------:|\n",
       "| **llm_score=0.0** |      201       |      121       |\n",
       "| **llm_score=1.0** |   57        |      570       |\n",
       "\n",
       "### ðŸ“Š Detailed Breakdown (Combined)\n",
       "\n",
       "| Category | Count | Percentage |\n",
       "|----------|------:|-----------:|\n",
       "| **Total samples** | 949 | 100.0% |\n",
       "| Both metrics = 0.0 | 201 | 21.2% |\n",
       "| Both metrics = 1.0 | 570 | 60.1% |\n",
       "| `llm_score`=1.0, `llm_judge_no_gt_v1_score`=0.0 | 57 | 6.0% |\n",
       "| `llm_score`=0.0, `llm_judge_no_gt_v1_score`=1.0 | 121 | 12.8% |\n",
       "\n",
       "### ðŸŽ¯ Agreement Analysis (Combined)\n",
       "\n",
       "- **Agreement:** 81.24% (771 out of 949 samples)\n",
       "- **Disagreement:** 18.76% (178 out of 949 samples)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  \"llm_judge_default_config_score\", \"llm_judge_alt_config_score\", \"llm_judge_no_gt_v1_score\", \"llm_judge_no_gt_v2_score\"\n",
    "key1 = \"llm_score\"  # \"llm_judge_default_config_score\" # \"llm_score\"\n",
    "key2 = \"llm_judge_no_gt_v1_score\"  # \"llm_judge_no_gt_v1_score\" # \"llm_judge_default_config_score\"\n",
    "\n",
    "confusion_matrix, agreement_percentage = print_confusion_matrix_markdown(\n",
    "    pred_eval_data,\n",
    "    key1,\n",
    "    key2,\n",
    "    # \"wxai:ibm/granite-3-3-8b-instruct-greedy-zero-shot-chatapi\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
